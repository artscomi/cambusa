# robots.txt file

# Allow all web crawlers full access
User-agent: *

# Disallow crawlers from accessing specific directories
Disallow: /sign-in*
Disallow: /group/create-group


